# 多题AI自动阅卷工具

## 简介

“多题AI自动阅卷工具”是一款基于Python和PyQt5开发的桌面应用程序，旨在帮助教育工作者或阅卷人员高效地完成多道题目的自动化批改工作。本工具通过集成先进的AI评分API，结合屏幕自动化操作（如鼠标点击、键盘输入和区域截图），实现了对学生手写答案的智能识别、评分和结果记录。它支持单题或多题的连续阅卷，并提供单评和双评两种模式，有效提升阅卷效率和准确性。

## 主要特性

*   **多题目支持**：可配置并自动处理多达4道题目，实现批量的自动化阅卷。
*   **单评与双评模式**：
    *   **单评模式**：使用一组AI API进行评分。
    *   **双评模式**：同时调用两组AI API进行独立评分，并计算分差。当分差超过预设阈值时，程序将自动中断并发出警告，提示人工介入复核，确保评分的严谨性。**注意：双评模式仅在只启用第一题时可用。**
*   **任务通知与声音提示**：在自动阅卷任务完成、因错误中断或双评分差过大时，程序会弹出简洁的通知窗口并播放系统提示音，及时提醒用户。通知窗口支持回车键确认，并在2分钟后重复播放声音，确保用户不会错过重要提示。
*   **配置即时更新与保存**：用户在UI界面上的所有修改都会被**即时更新到内存**中。在**启动自动阅卷任务前**或**关闭程序时**，所有内存中的配置项会被一次性保存到 `config.ini` 文件中，确保了操作的流畅性和数据的持久性。
*   **题目类型选择与精细化Prompt**：为每道题目提供多种预设的评分模式（如客观填空题、按点给分主观题、公式计算/证明题、整体评估开放题等）。每种模式都对应一个精心设计的、结构化的JSON Prompt，指导AI进行更准确的评分。该JSON Prompt包含通用的系统消息（强调阅卷总则，如涂改处理、严格依据细则、仅限图像内容、以及明确的评分原则与扣分规则）和针对特定题目类型的用户任务指令。用户任务指令详细定义了AI期望的JSON输出格式，包括`student_answer_summary`（学生答案摘要）、`scoring_basis`（详细评分依据）、`itemized_scores`（分项得分列表或总分）和`recognition_confidence`（手写识别可信度）。用户可根据题目特点选择最适合的模式。
*   **第一题三步打分模式**：针对第一题，可选择启用特殊的三步打分模式，将最终得分拆分为三部分输入到三个不同的位置，以适应特定的阅卷流程。**重要限制：此模式仅在只启用第一题时生效，与多题目模式互斥。**
*   **广泛的AI API兼容性**：内置**智能自适应API调用策略**，能够自动检测API类型、尝试多种请求格式和URL端点，并**缓存首次成功调用的策略**以提高后续效率。支持与多种主流AI服务商的视觉API进行交互，包括但不限于：
    *   OpenAI
    *   Azure OpenAI
    *   百度文心 (Ernie/Wenxin)
    *   智谱 (Zhipu/ChatGLM)
    *   阿里云 (Dashscope/Tongyi)
    *   火山引擎 (Volcengine/Doubao)
    *   腾讯混元 (Tencent/HunYuan)
    *   Moonshot
    *   Deepseek (深度求索)
    *   OpenRouter
    *   以及通用的OpenAI兼容格式API。
*   **屏幕自动化操作**：通过模拟鼠标点击、键盘输入和屏幕区域截图，实现与第三方阅卷系统或网页应用的无缝交互。
*   **灵活的题目配置**：每道题目均可独立配置，包括：
    *   启用/禁用状态。
    *   标准答案（评分细则）。
    *   分数范围（最低分、最高分）。
    *   分数输入框、提交按钮、翻页按钮的屏幕坐标。
    *   学生答案区域的精确框定（通过透明窗口辅助）。
*   **详细阅卷记录保存**：所有阅卷结果将自动保存为CSV格式文件，按日期和阅卷模式（单评/双评）分类存储。记录内容包括：
    *   **通用字段**：时间戳、记录类型 (`detail` 或 `summary`)、题目索引、总分、是否为双评运行批次、本次运行总题数。
    *   **单评模式**：学生答案摘要、详细评分依据、分项得分列表、手写识别可信度分数及理由。
    *   **双评模式**：分别记录两个API的学生答案摘要、评分依据、原始分数、分项得分列表、可信度分数及理由，以及实际分差和设定的分差阈值。
    *   **汇总记录**：包含总循环次数、尝试题数、完成题数、完成状态、中断原因、总用时、API模型ID等信息。
*   **实时日志与进度**：提供详细的运行日志和实时进度显示，方便用户监控阅卷过程和排查问题。
*   **健壮的错误处理与通知**：程序具备完善的错误捕获机制，并在任务完成、中断或发生错误时通过弹窗和系统声音进行及时通知。
*   **分数处理精度**：AI返回的原始分数会首先经过校验（确保在题目设定的最低分和最高分之间）；然后，此校验后的分数将被四舍五入到最接近的0.5的倍数；最后，这个经过0.5倍数处理的分数会再次被校验以确保其仍在题目配置的最低分和最高分范围内，最终用于输入。
*   **配置持久化**：所有用户配置（API密钥、坐标、题目设置等）将自动保存到 `setting/config.ini` 文件中，下次启动时自动加载。

## 技术栈

*   **Python 3.x**
*   **PyQt5**：用于构建图形用户界面。
*   **requests**：用于HTTP请求，与AI API进行通信。
*   **pyautogui**：用于自动化鼠标和键盘操作，实现屏幕交互。
*   **Pillow (PIL)**：用于图像处理和屏幕截图。
*   **configparser**：用于管理 `.ini` 格式的配置文件。
*   **csv**：用于读写CSV格式的阅卷记录。
*   **datetime, os, sys, traceback, base64, io, json, math**：标准库，用于日期时间处理、文件系统操作、错误追踪、Base64编码解码、内存文件操作、JSON处理、数学运算等。

## 安装与运行

### 环境准备

1.  **安装 Python 3.x**：
    建议安装 Python 3.8 或更高版本。您可以从 [Python 官方网站](https://www.python.org/downloads/) 下载并安装。
    在安装过程中，请确保勾选“Add Python to PATH”选项。

2.  **安装依赖库**：
    打开命令行工具（如 `cmd` 或 `PowerShell`），切换到项目根目录，然后运行以下命令安装所有必需的Python库：
    ```bash
    pip install PyQt5 requests pyautogui Pillow
    ```

### 运行程序

1.  **直接运行 Python 脚本**：
    在项目根目录（包含 `main.py` 的目录）下，打开命令行工具，运行：
    ```bash
    python main.py
    ```

2.  **运行打包后的可执行文件 (如果提供)**：
    如果项目提供了打包好的 `.exe` 文件（通常位于 `dist` 目录下），直接双击运行即可。

## 使用指南

### 界面概览

程序主界面分为几个主要区域：

*   **API 配置**：设置AI评分API的密钥、模型ID和URL。支持两组API配置。
*   **双评设置**：控制双评模式的启用与否，并设置分差阈值。
*   **科目选择**：选择当前阅卷的科目。此设置会影响AI评分时的系统提示词。
*   **自动化设置**：设置阅卷的循环次数和每次操作的等待时间。
*   **题目配置区**：显示多道题目的启用状态、标准答案输入框，以及用于打开详细配置对话框的按钮。
*   **操作按钮**：包括“自动运行”、“停止”和“API测试”按钮。
*   **日志区**：显示程序运行过程中的信息和错误日志。
*   **AI建议区**：显示AI返回的评分建议和详细理由（通常为JSON格式）。
*   **进度显示**：显示当前阅卷的进度。

### 配置步骤

在开始自动阅卷前，您需要根据实际情况进行详细配置。所有配置都会自动保存到 `setting/config.ini` 文件中。

#### 1. API 配置

在“API 配置”区域，填写您的AI评分API信息。
*   **第一个API**：用于单评模式和双评模式下的第一组评分。
    *   `API Key`：您的API密钥。
    *   `Model ID`：您使用的AI模型ID（例如 `gpt-4o-mini-vision`, `ERNIE-Bot-4` 等）。
    *   `API URL`：API的请求地址。
*   **第二个API**：仅在双评模式下使用。配置项与第一个API类似。

#### 2. 双评设置

*   **启用双评模式**：勾选“启用双评模式”复选框。
    *   **注意**：双评模式仅在**只启用第一题**时可用。如果启用了多道题目，双评模式将自动禁用。
*   **分差阈值**：设置两个API评分之间允许的最大分差。如果实际分差超过此值，程序将中断。

#### 3. 科目设置

*   在“科目”下拉菜单中选择或输入当前阅卷的科目（例如“语文”、“数学”）。这将影响AI Prompt中的通用系统消息，使其更具学科针对性。

#### 4. 自动化设置

*   **循环次数**：设置自动阅卷的批次数量。
*   **等待时间**：设置每次自动化操作（如输入分数、点击按钮）之间的等待秒数，以确保目标应用程序有足够时间响应。

#### 5. 题目类型选择 (核心Prompt逻辑)

在“选择该题 评分模式”下拉菜单中，为当前题目选择最适合的评分模式。不同的模式会向AI发送不同结构的JSON Prompt，指导其进行评分。所有Prompt都包含一个通用的系统消息（`system_message`）和针对具体任务的用户指令（`user_task`）。

**通用系统消息 (`system_message`) 结构示例** (科目会根据用户选择动态替换):
```
你是一位经验丰富、严谨细致的【{科目}】资深阅卷老师。
你的核心任务是：根据用户提供的【评分细则】和【题目类型说明】，对学生答案的图片内容进行深入分析和准确评分。
请你务必严格按照给定的JSON格式输出分析结果。

在整个评分过程中，请严格遵守以下【评分总则】：
1.  【关于涂改】：学生在答案文字上所作的任何横线、斜线、删除线或类似标记，均视为学生主动删除的内容。此部分内容不参与评分，即：既不因其正确而给分，也不因其可能存在的错误而不给分。请注意：学生可能会在原有答案涂改后，将新的答案写在涂改区域的旁边、上方或下方。如果这些补充内容清晰可辨且与题目相关，应将其视为学生最终意图表达的一部分，并纳入评分范围。
2.  【严格依据细则】：你的所有评分判断【必须且仅能严格依据】用户在【评分细则】中明确列出的每一个【得分点/答案要点/关键步骤/评估维度】（具体称呼依据题目类型而定）的标准和给分说明。严禁对评分细则进行任何形式的补充、推测、联想或超出细则范围进行给分或不给分。
3.  【仅限图像内容】：你的评分判断【仅能依据】从学生答题卡图片中真实可辨识的手写内容。严禁根据图片以外的任何信息（包括你对该学科知识的掌握、对评分细则的记忆或普遍常识）来猜测、臆断或虚构学生可能想表达的答案。
4.  【评分原则与扣分】：评分主要依据学生达到得分点的程度给分（即正面给分）。**然而，如果【评分细则】中明确包含“扣X分”的指令（例如：'每处过度解读扣0.5分'，'关键词误译扣2分'等），你必须严格执行这些扣分指令。** 请在`scoring_basis`的“判断与理由”中清晰说明扣分的原因和依据，并在最终的`得分`或`itemized_scores`中体现扣分后的结果。

【特殊情况处理】：
若学生答案图片完全空白、字迹完全无法辨认，或所写内容与题目要求完全无关，请按以下规则填充JSON：
    - `student_answer_summary`: 明确注明具体情况，例如：“学生未作答。”，“图片内容完全无法识别，字迹模糊不清。”，“学生答案内容与题目要求完全不符。”
    - `scoring_basis`: 简要说明此判断的依据，例如：“答题区域空白，无任何作答痕迹。”
    - `itemized_scores`:
        - 对于按【得分点/答案要点/关键步骤】给分的题型，应输出一个与【评分细则】中预设的相应条目数量相同长度的全零列表（例如，若细则有3个得分点，则输出 `[0, 0, 0]`）。
        - 对于整体评估的开放题型，应输出 `[0]`。
    - `recognition_confidence`: {"score": "1", "reason": "[对应上述特殊情况的理由，例如：图片空白或字迹完全无法识别。]"}
```

**用户任务 (`user_task`) 结构通用部分**:
`user_task` 包含以下通用字段，其具体内容会根据所选题目类型而变化：
*   `task_description`: 对AI任务的简要描述。
*   `question_type_specific_instructions`: 针对当前题目类型的详细评分指导。
*   `scoring_rubric_placeholder`: 用户在UI中输入的“标准答案/评分细则”将填充此处。
*   `student_answer_image_placeholder`: 逻辑占位符，实际图片通过API其他方式传入。
*   `output_format_specification`: **极其重要**，定义了AI必须严格遵守的JSON输出格式。

**期望的JSON输出格式 (`output_format_specification.format`) 通用结构**:
AI返回的JSON响应必须符合以下结构：
```json
{
  "student_answer_summary": "【请在此处对图片中的学生手写答案进行**中立、客观、不带任何主观判断**的【核心内容概括】...】",
  "scoring_basis": "【请针对【评分细则】中的【每一个...】，清晰地解释你是如何判断...以及得了多少分的...你需要：1. 引用或概括学生作答... 2. 结合评分细则，说明判断和理由... 3. 明确指出最终给了多少分...】",
  "itemized_scores": "[一个数字列表，例如 `[2, 0, 1]`。列表中的每个数字代表学生在【评分细则】中【对应顺序的每一个...】上获得的【实际得分】...]",
  "recognition_confidence": {
    "score": "[请从1-5中给出一个整数，代表你对本次图片中手写文字识别的自信程度...]",
    "reason": "[请用一句话简述你给出该分数的原因...]"
  }
}
```
**注意**：`scoring_basis` 的内容是一段详细的文本解释，说明了针对评分细则中每个点的判断依据和得分理由。`itemized_scores` 列表的含义和长度会根据题目类型而变化。

以下是各题目类型的具体说明和 `user_task` 中关键部分的示例：

##### 5.1 客观填空题 (Objective_FillInTheBlank)

*   **`question_type_specific_instructions`**: 强调对每个【填空项/答案要点】的精确判断，依据细则中的标准答案、允许表达方式及分值。
*   **`output_format_specification.format.scoring_basis`**: 详细解释对每个【填空项/答案要点】的评分判断和理由，包括引用学生作答、结合细则、明确得分。
*   **`output_format_specification.format.itemized_scores`**: 列表中的每个数字代表学生在【评分细则】中【对应顺序的每一个填空项/答案要点】上获得的【实际得分】。列表长度与评分细则中填空项/答案要点的数量一致。

##### 5.2 按点给分主观题 (Subjective_PointBased_QA)

*   **`question_type_specific_instructions`**: 强调判断学生答案是否覆盖评分细则中的各个【得分点】。
*   **`output_format_specification.format.scoring_basis`**: 详细解释对每个【得分点】的评分判断和理由，包括引用学生作答、结合细则、明确得分。
*   **`output_format_specification.format.itemized_scores`**: 列表中的每个数字代表学生在【评分细则】中【对应顺序的每一个得分点】上获得的【实际得分】。列表长度与评分细则中得分点的数量一致。

##### 5.3 公式计算/证明题 (Formula_Proof_StepBased)

*   **`question_type_specific_instructions`**: 强调核对解题的【每一个关键步骤/采分点】、公式准确性、计算结果、证明逻辑和书写规范。
*   **`output_format_specification.format.scoring_basis`**: 详细解释对每个【关键步骤/采分点】的评分判断和理由，包括引用学生作答、结合细则、明确得分。
*   **`output_format_specification.format.itemized_scores`**: 列表中的每个数字代表学生在【评分细则】中【对应顺序的每一个关键步骤/采分点】上获得的【实际得分】。列表长度与评分细则中关键步骤/采分点的数量一致。

##### 5.4 整体评估开放题 (Holistic_Evaluation_Open)

*   **`question_type_specific_instructions`**: 强调基于评分细则中的【各项评估维度/评分标准或等级描述】（如内容、结构、语言等）进行全面综合判断，给出最终总分。
*   **`output_format_specification.format.scoring_basis`**: 综合阐述给出最终总分的详细理由，参照各项整体评估维度，描述学生表现，并解释这些表现如何共同形成总分。
*   **`output_format_specification.format.itemized_scores`**: **一个【只包含一个数字的列表】**，这个数字代表根据【评分细则】中的整体评估标准/评分维度给出的【最终总分】。

#### 6. 题目配置（核心）

程序支持多达4道题目。每道题目都需要单独配置。

*   **启用题目**：
    *   默认第一题是启用的。
    *   勾选“启用第X题”复选框来启用其他题目。题目的启用是级联的，即只有前一题启用后，后一题才能被启用。
*   **标准答案**：在题目下方的文本框中输入该题目的详细评分细则和标准答案。这是AI评分的依据，请务必详细准确。
*   **分数范围**：设置该题目的最低分和最高分。
*   **打开题目配置对话框**：点击每道题目旁边的“配置第X题”按钮，将打开一个独立的配置对话框。

    ##### 题目配置对话框详解

    *   **分数输入位置**：
        1.  点击“设置分数输入位置”按钮。
        2.  程序会提示您将鼠标移动到目标应用程序中该题目的分数输入框位置。
        3.  5秒后，程序将自动捕获鼠标当前坐标并填充到X、Y文本框。
    *   **提交按钮位置**：
        1.  点击“设置提交按钮位置”按钮。
        2.  将鼠标移动到目标应用程序中该题目的“提交”或“确认”按钮位置。
        3.  5秒后，程序将自动捕获鼠标当前坐标并填充到X、Y文本框。
    *   **框定答案区域**：
        1.  点击“框定答案区域”按钮。
        2.  屏幕上将出现一个透明的红色边框窗口（“答案框”）。
        3.  **拖动和调整大小**：
            *   将鼠标移动到透明窗口的边缘，光标会变为调整大小的箭头，此时可以拖动边缘来调整窗口大小。
            *   将鼠标移动到透明窗口的内部，光标会变为普通箭头，此时可以拖动窗口来调整位置。
        4.  将此透明窗口精确覆盖目标应用程序中学生手写答案的区域。确保答案框完全包含学生答案，且不包含无关内容。
        5.  调整完成后，回到“题目配置对话框”，点击“确认框定”按钮。答案框的坐标将自动回填到对话框中，透明窗口将变为更透明的“已确认模式”并自动锁定。
        6.  如果需要重新框定，再次点击“框定答案区域”按钮。
    *   **翻页按钮配置**：
        1.  勾选“启用翻页按钮”复选框。
        2.  点击“设置翻页按钮位置”按钮。
        3.  将鼠标移动到目标应用程序中该题目的“下一题”或“翻页”按钮位置。
        4.  5秒后，程序将自动捕获鼠标当前坐标并填充到X、Y文本框。
        5.  启用此功能后，程序在完成当前题目的阅卷后，会自动点击此按钮进入下一题。
    *   **第一题三步打分模式（仅第一题）**：
        1.  在配置第一题时，您会看到“启用 最终得分分成三份 输入三个位置”复选框。
        2.  勾选此项后，原有的单点分数输入位置将被禁用，并出现三个新的分数输入位置设置。
        3.  分别点击“设置位置 1”、“设置位置 2”、“设置位置 3”按钮，并按照提示捕获屏幕上对应的三个分数输入框坐标。
        4.  **重要限制**：此模式仅在**只启用第一题**时生效。如果同时启用了其他题目，此功能将自动禁用。
    *   **保存配置**：在题目配置对话框中完成所有设置后，点击“保存”按钮。配置将更新到内存并自动保存到文件。

#### 7. 启动与停止阅卷

*   **启动**：点击主界面右下角的“自动运行”按钮。程序将开始执行自动化阅卷流程。
*   **停止**：点击主界面右下角的“停止”按钮，或按下键盘上的 `ESC` 键。程序将尝试安全中断当前的阅卷批次。

#### 8. API 连接测试

*   点击主界面右下角的“API测试”按钮。程序将尝试连接您配置的第一组API。如果启用了双评模式，也会测试第二组API。测试结果将通过弹窗显示。

### 阅卷记录

所有阅卷结果将保存在程序根目录下的 `阅卷记录/` 文件夹中。

*   **目录结构**：`阅卷记录/YYYYMMDD/`
    *   阅卷记录按日期创建子文件夹。
*   **文件命名**：`YYYY年MM月DD日_共X题_单评/双评.csv`
    *   文件名包含日期、题目数量和阅卷模式。
*   **CSV 文件内容**：
    *   **通用字段** (适用于所有记录): `timestamp` (时间戳), `record_type` (记录类型: `detail`表示详细记录, `summary`表示汇总记录), `question_index` (题目序号, 仅详细记录), `total_score` (最终得分, 仅详细记录), `is_dual_evaluation_run` (布尔值, 本次运行是否启用了双评模式), `total_questions_in_run` (本次运行配置的总题目数)。
    *   **单评模式 (详细记录 - `record_type: 'detail'`)**：
        *   `is_dual_evaluation` (布尔值, 对于单评模式下的详细记录，此值为 `False`)
        *   `student_answer` (学生答案摘要)
        *   `reasoning_basis` (详细评分依据文本)
        *   `sub_scores` (AI返回的分项得分列表，以字符串形式存储，例如 `"[1.0, 0.5, 2.0]"`)
        *   `confidence_score` (手写识别可信度分数, 1-5)
        *   `confidence_reason` (手写识别可信度理由)
    *   **双评模式 (详细记录 - `record_type: 'detail'`)**:
        *   `is_dual_evaluation` (布尔值, 对于双评模式下的详细记录，此值为 `True`)
        *   `api1_student_answer_summary` (第一个API的学生答案摘要)
        *   `api1_scoring_basis` (第一个API的详细评分依据文本)
        *   `api1_raw_score` (第一个API给出的原始总分)
        *   `api1_itemized_scores` (第一个API返回的分项得分列表，例如 `[2.0, 1.0]`)
        *   `api1_confidence_score` (第一个API的识别可信度分数)
        *   `api1_confidence_reason` (第一个API的识别可信度理由)
        *   `api2_student_answer_summary` (第二个API的学生答案摘要)
        *   `api2_scoring_basis` (第二个API的详细评分依据文本)
        *   `api2_raw_score` (第二个API给出的原始总分)
        *   `api2_itemized_scores` (第二个API返回的分项得分列表)
        *   `api2_confidence_score` (第二个API的识别可信度分数)
        *   `api2_confidence_reason` (第二个API的识别可信度理由)
        *   `score_difference` (两个API之间的原始总分分差)
        *   `score_diff_threshold` (设定的允许的最大分差阈值)
    *   **汇总记录 (`record_type: 'summary'`)**：每个阅卷批次结束后，会在CSV文件末尾添加一行。
        *   `total_cycles` (配置的总循环次数)
        *   `total_questions_attempted` (尝试的总题数 = `total_cycles` * `total_questions_in_run`)
        *   `questions_completed` (实际完成的题数)
        *   `completion_status` (完成状态: `completed`, `error`, `threshold_exceeded`)
        *   `interrupt_reason` (中断原因, 如果有)
        *   `total_elapsed_time_seconds` (总用时, 秒)
        *   `dual_evaluation_enabled` (本次运行是否启用了双评模式的全局设置)
        *   `first_model_id` (第一个API的模型ID)
        *   `second_model_id` (第二个API的模型ID, 如果双评启用)
        *   `is_single_question_one_run` (布尔值, 是否为仅运行第一题的模式)

## 注意事项

*   **运行环境要求**：程序依赖 `pyautogui` 进行屏幕自动化操作。因此，在程序运行期间，请确保：
    *   目标阅卷系统或网页窗口处于可见状态，且不被其他窗口遮挡或最小化。
    *   电脑屏幕保持解锁状态，不进入休眠或屏保。
    *   避免在程序运行期间进行其他鼠标或键盘操作，以免干扰自动化流程。
    *   避免在程序运行期间弹出任何可能遮挡目标窗口的系统或应用弹窗。
*   **屏幕分辨率与缩放**：坐标设置是基于当前屏幕分辨率和系统UI缩放比例的。如果更改屏幕分辨率或系统显示缩放比例，已设置的坐标可能失效，需要重新配置。
*   **API密钥安全**：请妥善保管您的API密钥，避免泄露。
*   **三步打分模式限制**：三步打分模式仅适用于第一题，且在非多题目阅卷模式下生效。如果同时启用了其他题目，此功能将自动禁用。
*   **网络连接**：AI评分API的调用需要稳定的互联网连接。
*   **程序中断**：在程序运行过程中，如果遇到API错误、网络中断、目标窗口被遮挡或用户手动停止，程序可能会中断。请根据日志信息进行排查。

## 常见问题 (FAQ)

**Q1: 为什么API连接测试失败？**
A1: 请检查以下几点：
    *   API Key、Model ID、API URL是否填写正确，没有多余的空格。
    *   您的API密钥是否有效或已过期。
    *   您的API账户是否有足够的余额。
    *   网络连接是否正常。
    *   API URL是否需要特定的前缀（如 `https://`）。

**Q2: 为什么程序无法提取分数或评分依据，或者AI建议区显示的JSON不符合预期？**
A2: 这通常是由于AI返回的响应格式不符合程序在特定题目类型下期望的JSON结构。请检查：
    *   您提供的“标准答案”（即评分细则，会作为Prompt的一部分发送给AI）是否清晰、完整，是否能有效引导AI按照“题目类型选择”部分描述的JSON格式输出（特别是 `student_answer_summary`, `scoring_basis`, `itemized_scores`, `recognition_confidence` 这几个关键字段）。
    *   确保AI模型具备理解并遵循复杂JSON格式指令的能力。部分能力较弱的模型可能难以严格遵循输出格式。
    *   尝试调整“标准答案”中的措辞或补充说明，以更好地引导AI。
    *   检查AI建议区显示的原始JSON响应，对比其与对应题目类型期望的输出格式，找出差异。特别注意 `scoring_basis` 是否为详细的文本解释，以及 `itemized_scores` 是否为数字列表。

**Q3: 程序运行过程中突然中断了，怎么办？**
A3: 请查看日志区（主界面右下角）的错误信息。常见原因包括：
    *   API调用失败（网络问题、API配额用尽、API服务商故障、API返回非JSON或无效JSON）。
    *   双评模式下分差超过阈值。
    *   目标窗口被遮挡、最小化或关闭。
    *   用户手动按 `ESC` 键或点击“停止”按钮。
    *   坐标设置不准确，导致鼠标点击或截图失败。
    *   评分细则为空或题目配置不完整。

**Q4: 坐标设置不准确，导致点击或截图位置不对？**
A4:
    *   确保在设置坐标时，目标应用程序窗口处于固定位置和大小，且没有被移动或缩放。
    *   确保您的系统显示缩放比例（DPI）设置为100%。如果不是，可能会导致坐标偏移。
    *   重新进入题目配置对话框，仔细使用“设置位置”和“框定答案区域”功能重新捕获坐标。

**Q5: 为什么我启用了第一题的三步打分，但程序运行时没有生效？**
A5: 三步打分模式仅在**只启用第一题**时生效。如果您同时启用了第二、三、四题中的任何一题，此功能将自动禁用。请确保只勾选了“启用第1题”复选框。
